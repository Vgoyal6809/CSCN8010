{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gym[atari] gym[accept-rom-license] ale-py\n"
      ],
      "metadata": {
        "id": "GaxIja6WbC5z",
        "outputId": "9c0b2c45-751a-4dd5-8f70-1236ae311b0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.10/dist-packages (0.7.5)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: autorom~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py) (6.4.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (4.66.6)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license]) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import datetime\n",
        "import random\n",
        "from collections import deque\n",
        "from tensorflow.keras import layers\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "y-u16d5PbDOz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "STATE_SHAPE = (84, 80)  # Height, Width\n",
        "ACTION_SIZE = 4  # Number of actions\n",
        "GAMMA = 0.99\n",
        "LEARNING_RATE = 0.0001\n",
        "EPSILON_INIT = 1.0\n",
        "EPSILON_MIN = 0.1\n",
        "EPSILON_DECAY = 0.995\n",
        "EPISODES = 500\n",
        "TARGET_UPDATE_RATE = 10\n",
        "MINI_BATCH_SIZE = 8\n",
        "REPLAY_BUFFER_CAPACITY = 50_000"
      ],
      "metadata": {
        "id": "m3mfD1UmbGQU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up environment\n",
        "env = gym.make('PongDeterministic-v4')\n"
      ],
      "metadata": {
        "id": "GecdHcB4bIvQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frame Preprocessing Functions\n",
        "def img_crop(img):\n",
        "    return img[30:-12, :, :]  # Crop irrelevant parts (specific to Pong)\n",
        "\n",
        "def downsample(img):\n",
        "    return img[::2, ::2]  # Downsample by a factor of 2\n",
        "\n",
        "def to_grayscale(img):\n",
        "    return np.mean(img, axis=2).astype(np.uint8)  # Convert to grayscale\n",
        "\n",
        "def normalize_grayscale(img):\n",
        "    return (img - 128) / 128 - 1  # Normalize grayscale from -1 to 1\n",
        "\n",
        "# Frame Preprocessing Functions\n",
        "def preprocess_frame(img, target_shape=(84, 80)):\n",
        "    \"\"\"Preprocess the input frame.\"\"\"\n",
        "    img = img[30:-12, :, :]  # Crop to remove irrelevant parts\n",
        "    img = img[::2, ::2]  # Downsample\n",
        "    img = np.mean(img, axis=2).astype(np.uint8)  # Grayscale\n",
        "    img = (img - 128) / 128 - 1  # Normalize\n",
        "    img = cv2.resize(img, target_shape, interpolation=cv2.INTER_AREA)\n",
        "    return np.expand_dims(img, axis=-1)  # Shape: (84, 80, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "9Q6DW8jabJBF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replay Buffer Class\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def store(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n"
      ],
      "metadata": {
        "id": "aYGYbFAxbKma"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "ypkJsvRBbMW8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN Model Builder\n",
        "def build_model(input_shape, action_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (8, 8), strides=4, activation='relu', input_shape=(input_shape[0], input_shape[1], 4)),\n",
        "        tf.keras.layers.Conv2D(64, (4, 4), strides=2, activation='relu'),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), strides=1, activation='relu'),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(action_size, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='mse')\n",
        "    return model"
      ],
      "metadata": {
        "id": "ehgwdpGrbO9t"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def epsilon_greedy_action(state, model, epsilon):\n",
        "    \"\"\"Select an action using epsilon-greedy strategy.\"\"\"\n",
        "    # Ensure epsilon is a TensorFlow float\n",
        "    epsilon = tf.convert_to_tensor(epsilon, dtype=tf.float32)\n",
        "    # Generate a random value\n",
        "    random_val = tf.random.uniform([], dtype=tf.float32)\n",
        "    if random_val < epsilon:\n",
        "        # Explore: choose a random action\n",
        "        return tf.random.uniform([], 0, ACTION_SIZE, dtype=tf.int32)\n",
        "    else:\n",
        "        # Exploit: choose the best action\n",
        "        q_values = model(state, training=False)\n",
        "        return tf.argmax(q_values[0], axis=-1, output_type=tf.int32)\n",
        "\n",
        "\n",
        "# Training on Mini-Batches\n",
        "@tf.function\n",
        "def train_minibatch(states, actions, rewards, next_states, dones, policy_net, target_net):\n",
        "    next_q_values = target_net(next_states, training=False)\n",
        "    max_next_q_values = tf.reduce_max(next_q_values, axis=1)\n",
        "    target_q_values = rewards + (1 - dones) * GAMMA * max_next_q_values\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        q_values = policy_net(states, training=True)\n",
        "        q_values_taken = tf.reduce_sum(q_values * tf.one_hot(actions, ACTION_SIZE), axis=1)\n",
        "        loss = tf.reduce_mean(tf.square(target_q_values - q_values_taken))\n",
        "\n",
        "    gradients = tape.gradient(loss, policy_net.trainable_variables)\n",
        "    policy_net.optimizer.apply_gradients(zip(gradients, policy_net.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# Initialize Models, Replay Buffer, and Epsilon\n",
        "policy_net = build_model(STATE_SHAPE, ACTION_SIZE)\n",
        "target_net = build_model(STATE_SHAPE, ACTION_SIZE)\n",
        "target_net.set_weights(policy_net.get_weights())\n",
        "\n",
        "replay_buffer = ReplayBuffer(REPLAY_BUFFER_CAPACITY)\n",
        "epsilon = EPSILON_INIT\n"
      ],
      "metadata": {
        "id": "qUMJXtSpbPVF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "rewards_per_episode = []\n",
        "for episode in range(EPISODES):\n",
        "    state = env.reset()  # This may return just the state or a dictionary\n",
        "    if isinstance(state, dict):\n",
        "        state = state['state']  # If it's a dictionary, extract the 'state' field\n",
        "\n",
        "    state = preprocess_frame(state)  # Process the state frame\n",
        "    state_stack = np.repeat(state, 4, axis=-1)  # Stack 4 frames\n",
        "\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = epsilon_greedy_action(np.expand_dims(state_stack, axis=0), policy_net, epsilon)\n",
        "        next_state, reward, done, _ = env.step(int(action))\n",
        "        next_state = preprocess_frame(next_state)\n",
        "        next_state_stack = np.append(state_stack[:, :, 1:], next_state, axis=-1)\n",
        "\n",
        "        replay_buffer.store((state_stack, int(action), reward, next_state_stack, done))\n",
        "        state_stack = next_state_stack\n",
        "        total_reward += reward\n",
        "\n",
        "        if replay_buffer.size() >= MINI_BATCH_SIZE:\n",
        "            minibatch = replay_buffer.sample(MINI_BATCH_SIZE)\n",
        "            states, actions, rewards, next_states, dones = map(np.array, zip(*minibatch))\n",
        "            rewards = rewards.astype(np.float32)\n",
        "            dones = dones.astype(np.float32)\n",
        "            train_minibatch(states, actions, rewards, next_states, dones, policy_net, target_net)\n",
        "\n",
        "    rewards_per_episode.append(total_reward)\n",
        "    avg_reward = np.mean(rewards_per_episode[-5:])\n",
        "\n",
        "    # with train_summary_writer.as_default():\n",
        "    #     tf.summary.scalar('Total Reward', total_reward, step=episode)\n",
        "    #     tf.summary.scalar('Average Reward (Last 5)', avg_reward, step=episode)\n",
        "    #     tf.summary.scalar('Epsilon', epsilon, step=episode)\n",
        "\n",
        "    if episode % TARGET_UPDATE_RATE == 0:\n",
        "        target_net.set_weights(policy_net.get_weights())\n",
        "\n",
        "    epsilon = max(EPSILON_MIN, epsilon * EPSILON_DECAY)\n",
        "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Avg Reward = {avg_reward:.2f}, Epsilon = {epsilon:.3f}\")\n",
        "\n",
        "# Plot Rewards\n",
        "plt.plot(rewards_per_episode)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Rewards Over Episodes')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rtxYJHDEbQmB",
        "outputId": "46b39de7-abe0-4729-93b7-f74621788d5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.995\n",
            "Episode 2: Total Reward = -20.0, Avg Reward = -20.50, Epsilon = 0.990\n",
            "Episode 3: Total Reward = -21.0, Avg Reward = -20.67, Epsilon = 0.985\n",
            "Episode 4: Total Reward = -20.0, Avg Reward = -20.50, Epsilon = 0.980\n",
            "Episode 5: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.975\n",
            "Episode 6: Total Reward = -18.0, Avg Reward = -20.00, Epsilon = 0.970\n",
            "Episode 7: Total Reward = -21.0, Avg Reward = -20.20, Epsilon = 0.966\n",
            "Episode 8: Total Reward = -20.0, Avg Reward = -20.00, Epsilon = 0.961\n",
            "Episode 9: Total Reward = -20.0, Avg Reward = -20.00, Epsilon = 0.956\n",
            "Episode 10: Total Reward = -19.0, Avg Reward = -19.60, Epsilon = 0.951\n",
            "Episode 11: Total Reward = -20.0, Avg Reward = -20.00, Epsilon = 0.946\n",
            "Episode 12: Total Reward = -21.0, Avg Reward = -20.00, Epsilon = 0.942\n",
            "Episode 13: Total Reward = -21.0, Avg Reward = -20.20, Epsilon = 0.937\n",
            "Episode 14: Total Reward = -21.0, Avg Reward = -20.40, Epsilon = 0.932\n",
            "Episode 15: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.928\n",
            "Episode 16: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.923\n",
            "Episode 17: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.918\n",
            "Episode 18: Total Reward = -20.0, Avg Reward = -20.80, Epsilon = 0.914\n",
            "Episode 19: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.909\n",
            "Episode 20: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.905\n",
            "Episode 21: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.900\n",
            "Episode 22: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.896\n",
            "Episode 23: Total Reward = -19.0, Avg Reward = -20.60, Epsilon = 0.891\n",
            "Episode 24: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.887\n",
            "Episode 25: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.882\n",
            "Episode 26: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.878\n",
            "Episode 27: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.873\n",
            "Episode 28: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.869\n",
            "Episode 29: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.865\n",
            "Episode 30: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.860\n",
            "Episode 31: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.856\n",
            "Episode 32: Total Reward = -20.0, Avg Reward = -20.80, Epsilon = 0.852\n",
            "Episode 33: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.848\n",
            "Episode 34: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.843\n",
            "Episode 35: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.839\n",
            "Episode 36: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.835\n",
            "Episode 37: Total Reward = -20.0, Avg Reward = -20.80, Epsilon = 0.831\n",
            "Episode 38: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.827\n",
            "Episode 39: Total Reward = -20.0, Avg Reward = -20.40, Epsilon = 0.822\n",
            "Episode 40: Total Reward = -21.0, Avg Reward = -20.40, Epsilon = 0.818\n",
            "Episode 41: Total Reward = -19.0, Avg Reward = -20.00, Epsilon = 0.814\n",
            "Episode 42: Total Reward = -21.0, Avg Reward = -20.20, Epsilon = 0.810\n",
            "Episode 43: Total Reward = -20.0, Avg Reward = -20.20, Epsilon = 0.806\n",
            "Episode 44: Total Reward = -21.0, Avg Reward = -20.40, Epsilon = 0.802\n",
            "Episode 45: Total Reward = -21.0, Avg Reward = -20.40, Epsilon = 0.798\n",
            "Episode 46: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.794\n",
            "Episode 47: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.790\n",
            "Episode 48: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.786\n",
            "Episode 49: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.782\n",
            "Episode 50: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.778\n",
            "Episode 51: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.774\n",
            "Episode 52: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.771\n",
            "Episode 53: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.767\n",
            "Episode 54: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.763\n",
            "Episode 55: Total Reward = -19.0, Avg Reward = -20.60, Epsilon = 0.759\n",
            "Episode 56: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.755\n",
            "Episode 57: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.751\n",
            "Episode 58: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.748\n",
            "Episode 59: Total Reward = -20.0, Avg Reward = -20.40, Epsilon = 0.744\n",
            "Episode 60: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.740\n",
            "Episode 61: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.737\n",
            "Episode 62: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.733\n",
            "Episode 63: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.729\n",
            "Episode 64: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.726\n",
            "Episode 65: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.722\n",
            "Episode 66: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.718\n",
            "Episode 67: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.715\n",
            "Episode 68: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.711\n",
            "Episode 69: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.708\n",
            "Episode 70: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.704\n",
            "Episode 71: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.701\n",
            "Episode 72: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.697\n",
            "Episode 73: Total Reward = -20.0, Avg Reward = -20.80, Epsilon = 0.694\n",
            "Episode 74: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.690\n",
            "Episode 75: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.687\n",
            "Episode 76: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.683\n",
            "Episode 77: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.680\n",
            "Episode 78: Total Reward = -20.0, Avg Reward = -20.80, Epsilon = 0.676\n",
            "Episode 79: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.673\n",
            "Episode 80: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.670\n",
            "Episode 81: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.666\n",
            "Episode 82: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.663\n",
            "Episode 83: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.660\n",
            "Episode 84: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.656\n",
            "Episode 85: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.653\n",
            "Episode 86: Total Reward = -20.0, Avg Reward = -20.80, Epsilon = 0.650\n",
            "Episode 87: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.647\n",
            "Episode 88: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.643\n",
            "Episode 89: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.640\n",
            "Episode 90: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.637\n",
            "Episode 91: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.634\n",
            "Episode 92: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.631\n",
            "Episode 93: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.627\n",
            "Episode 94: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.624\n",
            "Episode 95: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.621\n",
            "Episode 96: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.618\n",
            "Episode 97: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.615\n",
            "Episode 98: Total Reward = -20.0, Avg Reward = -20.80, Epsilon = 0.612\n",
            "Episode 99: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.609\n",
            "Episode 100: Total Reward = -19.0, Avg Reward = -20.20, Epsilon = 0.606\n",
            "Episode 101: Total Reward = -21.0, Avg Reward = -20.20, Epsilon = 0.603\n",
            "Episode 102: Total Reward = -21.0, Avg Reward = -20.20, Epsilon = 0.600\n",
            "Episode 103: Total Reward = -21.0, Avg Reward = -20.40, Epsilon = 0.597\n",
            "Episode 104: Total Reward = -20.0, Avg Reward = -20.40, Epsilon = 0.594\n",
            "Episode 105: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.591\n",
            "Episode 106: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.588\n",
            "Episode 107: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.585\n",
            "Episode 108: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.582\n",
            "Episode 109: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.579\n",
            "Episode 110: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.576\n",
            "Episode 111: Total Reward = -21.0, Avg Reward = -21.00, Epsilon = 0.573\n",
            "Episode 112: Total Reward = -20.0, Avg Reward = -20.80, Epsilon = 0.570\n",
            "Episode 113: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.568\n",
            "Episode 114: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.565\n",
            "Episode 115: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.562\n",
            "Episode 116: Total Reward = -20.0, Avg Reward = -20.40, Epsilon = 0.559\n",
            "Episode 117: Total Reward = -20.0, Avg Reward = -20.40, Epsilon = 0.556\n",
            "Episode 118: Total Reward = -21.0, Avg Reward = -20.40, Epsilon = 0.554\n",
            "Episode 119: Total Reward = -21.0, Avg Reward = -20.40, Epsilon = 0.551\n",
            "Episode 120: Total Reward = -20.0, Avg Reward = -20.40, Epsilon = 0.548\n",
            "Episode 121: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.545\n",
            "Episode 122: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.543\n",
            "Episode 123: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.540\n",
            "Episode 124: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.537\n",
            "Episode 125: Total Reward = -18.0, Avg Reward = -20.40, Epsilon = 0.534\n",
            "Episode 126: Total Reward = -18.0, Avg Reward = -19.80, Epsilon = 0.532\n",
            "Episode 127: Total Reward = -20.0, Avg Reward = -19.60, Epsilon = 0.529\n",
            "Episode 128: Total Reward = -21.0, Avg Reward = -19.60, Epsilon = 0.526\n",
            "Episode 129: Total Reward = -21.0, Avg Reward = -19.60, Epsilon = 0.524\n",
            "Episode 130: Total Reward = -21.0, Avg Reward = -20.20, Epsilon = 0.521\n",
            "Episode 131: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.519\n",
            "Episode 132: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.516\n",
            "Episode 133: Total Reward = -20.0, Avg Reward = -20.40, Epsilon = 0.513\n",
            "Episode 134: Total Reward = -21.0, Avg Reward = -20.40, Epsilon = 0.511\n",
            "Episode 135: Total Reward = -21.0, Avg Reward = -20.40, Epsilon = 0.508\n",
            "Episode 136: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.506\n",
            "Episode 137: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.503\n",
            "Episode 138: Total Reward = -20.0, Avg Reward = -20.60, Epsilon = 0.501\n",
            "Episode 139: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.498\n",
            "Episode 140: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.496\n",
            "Episode 141: Total Reward = -18.0, Avg Reward = -20.00, Epsilon = 0.493\n",
            "Episode 142: Total Reward = -20.0, Avg Reward = -20.00, Epsilon = 0.491\n",
            "Episode 143: Total Reward = -20.0, Avg Reward = -20.00, Epsilon = 0.488\n",
            "Episode 144: Total Reward = -20.0, Avg Reward = -19.80, Epsilon = 0.486\n",
            "Episode 145: Total Reward = -21.0, Avg Reward = -19.80, Epsilon = 0.483\n",
            "Episode 146: Total Reward = -21.0, Avg Reward = -20.40, Epsilon = 0.481\n",
            "Episode 147: Total Reward = -20.0, Avg Reward = -20.40, Epsilon = 0.479\n",
            "Episode 148: Total Reward = -21.0, Avg Reward = -20.60, Epsilon = 0.476\n",
            "Episode 149: Total Reward = -21.0, Avg Reward = -20.80, Epsilon = 0.474\n",
            "Episode 150: Total Reward = -19.0, Avg Reward = -20.40, Epsilon = 0.471\n",
            "Episode 151: Total Reward = -21.0, Avg Reward = -20.40, Epsilon = 0.469\n",
            "Episode 152: Total Reward = -19.0, Avg Reward = -20.20, Epsilon = 0.467\n",
            "Episode 153: Total Reward = -20.0, Avg Reward = -20.00, Epsilon = 0.464\n",
            "Episode 154: Total Reward = -19.0, Avg Reward = -19.60, Epsilon = 0.462\n",
            "Episode 155: Total Reward = -17.0, Avg Reward = -19.20, Epsilon = 0.460\n",
            "Episode 156: Total Reward = -16.0, Avg Reward = -18.20, Epsilon = 0.458\n",
            "Episode 157: Total Reward = -21.0, Avg Reward = -18.60, Epsilon = 0.455\n",
            "Episode 158: Total Reward = -20.0, Avg Reward = -18.60, Epsilon = 0.453\n",
            "Episode 159: Total Reward = -21.0, Avg Reward = -19.00, Epsilon = 0.451\n",
            "Episode 160: Total Reward = -19.0, Avg Reward = -19.40, Epsilon = 0.448\n",
            "Episode 161: Total Reward = -19.0, Avg Reward = -20.00, Epsilon = 0.446\n",
            "Episode 162: Total Reward = -21.0, Avg Reward = -20.00, Epsilon = 0.444\n",
            "Episode 163: Total Reward = -19.0, Avg Reward = -19.80, Epsilon = 0.442\n",
            "Episode 164: Total Reward = -17.0, Avg Reward = -19.00, Epsilon = 0.440\n",
            "Episode 165: Total Reward = -21.0, Avg Reward = -19.40, Epsilon = 0.437\n",
            "Episode 166: Total Reward = -20.0, Avg Reward = -19.60, Epsilon = 0.435\n",
            "Episode 167: Total Reward = -21.0, Avg Reward = -19.60, Epsilon = 0.433\n",
            "Episode 168: Total Reward = -19.0, Avg Reward = -19.60, Epsilon = 0.431\n",
            "Episode 169: Total Reward = -20.0, Avg Reward = -20.20, Epsilon = 0.429\n",
            "Episode 170: Total Reward = -21.0, Avg Reward = -20.20, Epsilon = 0.427\n",
            "Episode 171: Total Reward = -19.0, Avg Reward = -20.00, Epsilon = 0.424\n",
            "Episode 172: Total Reward = -17.0, Avg Reward = -19.20, Epsilon = 0.422\n",
            "Episode 173: Total Reward = -20.0, Avg Reward = -19.40, Epsilon = 0.420\n",
            "Episode 174: Total Reward = -19.0, Avg Reward = -19.20, Epsilon = 0.418\n",
            "Episode 175: Total Reward = -20.0, Avg Reward = -19.00, Epsilon = 0.416\n",
            "Episode 176: Total Reward = -19.0, Avg Reward = -19.00, Epsilon = 0.414\n",
            "Episode 177: Total Reward = -19.0, Avg Reward = -19.40, Epsilon = 0.412\n",
            "Episode 178: Total Reward = -17.0, Avg Reward = -18.80, Epsilon = 0.410\n",
            "Episode 179: Total Reward = -19.0, Avg Reward = -18.80, Epsilon = 0.408\n",
            "Episode 180: Total Reward = -18.0, Avg Reward = -18.40, Epsilon = 0.406\n",
            "Episode 181: Total Reward = -21.0, Avg Reward = -18.80, Epsilon = 0.404\n",
            "Episode 182: Total Reward = -20.0, Avg Reward = -19.00, Epsilon = 0.402\n",
            "Episode 183: Total Reward = -20.0, Avg Reward = -19.60, Epsilon = 0.400\n",
            "Episode 184: Total Reward = -20.0, Avg Reward = -19.80, Epsilon = 0.398\n",
            "Episode 185: Total Reward = -16.0, Avg Reward = -19.40, Epsilon = 0.396\n",
            "Episode 186: Total Reward = -18.0, Avg Reward = -18.80, Epsilon = 0.394\n",
            "Episode 187: Total Reward = -20.0, Avg Reward = -18.80, Epsilon = 0.392\n",
            "Episode 188: Total Reward = -19.0, Avg Reward = -18.60, Epsilon = 0.390\n",
            "Episode 189: Total Reward = -19.0, Avg Reward = -18.40, Epsilon = 0.388\n",
            "Episode 190: Total Reward = -19.0, Avg Reward = -19.00, Epsilon = 0.386\n",
            "Episode 191: Total Reward = -19.0, Avg Reward = -19.20, Epsilon = 0.384\n",
            "Episode 192: Total Reward = -18.0, Avg Reward = -18.80, Epsilon = 0.382\n",
            "Episode 193: Total Reward = -18.0, Avg Reward = -18.60, Epsilon = 0.380\n",
            "Episode 194: Total Reward = -16.0, Avg Reward = -18.00, Epsilon = 0.378\n",
            "Episode 195: Total Reward = -18.0, Avg Reward = -17.80, Epsilon = 0.376\n",
            "Episode 196: Total Reward = -18.0, Avg Reward = -17.60, Epsilon = 0.374\n",
            "Episode 197: Total Reward = -20.0, Avg Reward = -18.00, Epsilon = 0.373\n",
            "Episode 198: Total Reward = -20.0, Avg Reward = -18.40, Epsilon = 0.371\n",
            "Episode 199: Total Reward = -18.0, Avg Reward = -18.80, Epsilon = 0.369\n",
            "Episode 200: Total Reward = -21.0, Avg Reward = -19.40, Epsilon = 0.367\n",
            "Episode 201: Total Reward = -18.0, Avg Reward = -19.40, Epsilon = 0.365\n",
            "Episode 202: Total Reward = -19.0, Avg Reward = -19.20, Epsilon = 0.363\n",
            "Episode 203: Total Reward = -17.0, Avg Reward = -18.60, Epsilon = 0.361\n",
            "Episode 204: Total Reward = -17.0, Avg Reward = -18.40, Epsilon = 0.360\n",
            "Episode 205: Total Reward = -16.0, Avg Reward = -17.40, Epsilon = 0.358\n",
            "Episode 206: Total Reward = -16.0, Avg Reward = -17.00, Epsilon = 0.356\n",
            "Episode 207: Total Reward = -16.0, Avg Reward = -16.40, Epsilon = 0.354\n",
            "Episode 208: Total Reward = -21.0, Avg Reward = -17.20, Epsilon = 0.353\n",
            "Episode 209: Total Reward = -20.0, Avg Reward = -17.80, Epsilon = 0.351\n",
            "Episode 210: Total Reward = -21.0, Avg Reward = -18.80, Epsilon = 0.349\n",
            "Episode 211: Total Reward = -19.0, Avg Reward = -19.40, Epsilon = 0.347\n",
            "Episode 212: Total Reward = -16.0, Avg Reward = -19.40, Epsilon = 0.346\n",
            "Episode 213: Total Reward = -19.0, Avg Reward = -19.00, Epsilon = 0.344\n",
            "Episode 214: Total Reward = -19.0, Avg Reward = -18.80, Epsilon = 0.342\n",
            "Episode 215: Total Reward = -18.0, Avg Reward = -18.20, Epsilon = 0.340\n",
            "Episode 216: Total Reward = -11.0, Avg Reward = -16.60, Epsilon = 0.339\n",
            "Episode 217: Total Reward = -18.0, Avg Reward = -17.00, Epsilon = 0.337\n",
            "Episode 218: Total Reward = -17.0, Avg Reward = -16.60, Epsilon = 0.335\n",
            "Episode 219: Total Reward = -14.0, Avg Reward = -15.60, Epsilon = 0.334\n",
            "Episode 220: Total Reward = -15.0, Avg Reward = -15.00, Epsilon = 0.332\n",
            "Episode 221: Total Reward = -17.0, Avg Reward = -16.20, Epsilon = 0.330\n",
            "Episode 222: Total Reward = -16.0, Avg Reward = -15.80, Epsilon = 0.329\n",
            "Episode 223: Total Reward = -19.0, Avg Reward = -16.20, Epsilon = 0.327\n",
            "Episode 224: Total Reward = -16.0, Avg Reward = -16.60, Epsilon = 0.325\n",
            "Episode 225: Total Reward = -16.0, Avg Reward = -16.80, Epsilon = 0.324\n",
            "Episode 226: Total Reward = -15.0, Avg Reward = -16.40, Epsilon = 0.322\n",
            "Episode 227: Total Reward = -19.0, Avg Reward = -17.00, Epsilon = 0.321\n",
            "Episode 228: Total Reward = -13.0, Avg Reward = -15.80, Epsilon = 0.319\n",
            "Episode 229: Total Reward = -18.0, Avg Reward = -16.20, Epsilon = 0.317\n",
            "Episode 230: Total Reward = -17.0, Avg Reward = -16.40, Epsilon = 0.316\n",
            "Episode 231: Total Reward = -18.0, Avg Reward = -17.00, Epsilon = 0.314\n",
            "Episode 232: Total Reward = -17.0, Avg Reward = -16.60, Epsilon = 0.313\n",
            "Episode 233: Total Reward = -16.0, Avg Reward = -17.20, Epsilon = 0.311\n",
            "Episode 234: Total Reward = -20.0, Avg Reward = -17.60, Epsilon = 0.309\n",
            "Episode 235: Total Reward = -12.0, Avg Reward = -16.60, Epsilon = 0.308\n",
            "Episode 236: Total Reward = -13.0, Avg Reward = -15.60, Epsilon = 0.306\n",
            "Episode 237: Total Reward = -18.0, Avg Reward = -15.80, Epsilon = 0.305\n",
            "Episode 238: Total Reward = -14.0, Avg Reward = -15.40, Epsilon = 0.303\n",
            "Episode 239: Total Reward = -18.0, Avg Reward = -15.00, Epsilon = 0.302\n",
            "Episode 240: Total Reward = -16.0, Avg Reward = -15.80, Epsilon = 0.300\n",
            "Episode 241: Total Reward = -17.0, Avg Reward = -16.60, Epsilon = 0.299\n",
            "Episode 242: Total Reward = -9.0, Avg Reward = -14.80, Epsilon = 0.297\n",
            "Episode 243: Total Reward = -14.0, Avg Reward = -14.80, Epsilon = 0.296\n",
            "Episode 244: Total Reward = -11.0, Avg Reward = -13.40, Epsilon = 0.294\n",
            "Episode 245: Total Reward = -13.0, Avg Reward = -12.80, Epsilon = 0.293\n",
            "Episode 246: Total Reward = -13.0, Avg Reward = -12.00, Epsilon = 0.291\n",
            "Episode 247: Total Reward = -19.0, Avg Reward = -14.00, Epsilon = 0.290\n",
            "Episode 248: Total Reward = -20.0, Avg Reward = -15.20, Epsilon = 0.288\n",
            "Episode 249: Total Reward = -14.0, Avg Reward = -15.80, Epsilon = 0.287\n",
            "Episode 250: Total Reward = -19.0, Avg Reward = -17.00, Epsilon = 0.286\n",
            "Episode 251: Total Reward = -20.0, Avg Reward = -18.40, Epsilon = 0.284\n",
            "Episode 252: Total Reward = -15.0, Avg Reward = -17.60, Epsilon = 0.283\n",
            "Episode 253: Total Reward = -16.0, Avg Reward = -16.80, Epsilon = 0.281\n",
            "Episode 254: Total Reward = -14.0, Avg Reward = -16.80, Epsilon = 0.280\n",
            "Episode 255: Total Reward = -16.0, Avg Reward = -16.20, Epsilon = 0.279\n",
            "Episode 256: Total Reward = -18.0, Avg Reward = -15.80, Epsilon = 0.277\n",
            "Episode 257: Total Reward = -16.0, Avg Reward = -16.00, Epsilon = 0.276\n",
            "Episode 258: Total Reward = -14.0, Avg Reward = -15.60, Epsilon = 0.274\n",
            "Episode 259: Total Reward = -20.0, Avg Reward = -16.80, Epsilon = 0.273\n",
            "Episode 260: Total Reward = -11.0, Avg Reward = -15.80, Epsilon = 0.272\n",
            "Episode 261: Total Reward = -16.0, Avg Reward = -15.40, Epsilon = 0.270\n",
            "Episode 262: Total Reward = -17.0, Avg Reward = -15.60, Epsilon = 0.269\n",
            "Episode 263: Total Reward = -14.0, Avg Reward = -15.60, Epsilon = 0.268\n",
            "Episode 264: Total Reward = -17.0, Avg Reward = -15.00, Epsilon = 0.266\n",
            "Episode 265: Total Reward = -14.0, Avg Reward = -15.60, Epsilon = 0.265\n",
            "Episode 266: Total Reward = -17.0, Avg Reward = -15.80, Epsilon = 0.264\n",
            "Episode 267: Total Reward = -17.0, Avg Reward = -15.80, Epsilon = 0.262\n",
            "Episode 268: Total Reward = -14.0, Avg Reward = -15.80, Epsilon = 0.261\n",
            "Episode 269: Total Reward = -16.0, Avg Reward = -15.60, Epsilon = 0.260\n",
            "Episode 270: Total Reward = -11.0, Avg Reward = -15.00, Epsilon = 0.258\n",
            "Episode 271: Total Reward = -12.0, Avg Reward = -14.00, Epsilon = 0.257\n",
            "Episode 272: Total Reward = -15.0, Avg Reward = -13.60, Epsilon = 0.256\n",
            "Episode 273: Total Reward = -16.0, Avg Reward = -14.00, Epsilon = 0.255\n",
            "Episode 274: Total Reward = -14.0, Avg Reward = -13.60, Epsilon = 0.253\n",
            "Episode 275: Total Reward = -17.0, Avg Reward = -14.80, Epsilon = 0.252\n",
            "Episode 276: Total Reward = -13.0, Avg Reward = -15.00, Epsilon = 0.251\n",
            "Episode 277: Total Reward = -18.0, Avg Reward = -15.60, Epsilon = 0.249\n",
            "Episode 278: Total Reward = -14.0, Avg Reward = -15.20, Epsilon = 0.248\n",
            "Episode 279: Total Reward = -16.0, Avg Reward = -15.60, Epsilon = 0.247\n",
            "Episode 280: Total Reward = -9.0, Avg Reward = -14.00, Epsilon = 0.246\n",
            "Episode 281: Total Reward = -14.0, Avg Reward = -14.20, Epsilon = 0.245\n",
            "Episode 282: Total Reward = -11.0, Avg Reward = -12.80, Epsilon = 0.243\n",
            "Episode 283: Total Reward = -7.0, Avg Reward = -11.40, Epsilon = 0.242\n",
            "Episode 284: Total Reward = -15.0, Avg Reward = -11.20, Epsilon = 0.241\n",
            "Episode 285: Total Reward = -17.0, Avg Reward = -12.80, Epsilon = 0.240\n",
            "Episode 286: Total Reward = -17.0, Avg Reward = -13.40, Epsilon = 0.238\n",
            "Episode 287: Total Reward = -16.0, Avg Reward = -14.40, Epsilon = 0.237\n",
            "Episode 288: Total Reward = -16.0, Avg Reward = -16.20, Epsilon = 0.236\n",
            "Episode 289: Total Reward = -14.0, Avg Reward = -16.00, Epsilon = 0.235\n",
            "Episode 290: Total Reward = -13.0, Avg Reward = -15.20, Epsilon = 0.234\n",
            "Episode 291: Total Reward = -18.0, Avg Reward = -15.40, Epsilon = 0.233\n",
            "Episode 292: Total Reward = -12.0, Avg Reward = -14.60, Epsilon = 0.231\n",
            "Episode 293: Total Reward = -11.0, Avg Reward = -13.60, Epsilon = 0.230\n",
            "Episode 294: Total Reward = -14.0, Avg Reward = -13.60, Epsilon = 0.229\n",
            "Episode 295: Total Reward = -14.0, Avg Reward = -13.80, Epsilon = 0.228\n",
            "Episode 296: Total Reward = -13.0, Avg Reward = -12.80, Epsilon = 0.227\n",
            "Episode 297: Total Reward = -14.0, Avg Reward = -13.20, Epsilon = 0.226\n",
            "Episode 298: Total Reward = -18.0, Avg Reward = -14.60, Epsilon = 0.225\n",
            "Episode 299: Total Reward = -19.0, Avg Reward = -15.60, Epsilon = 0.223\n",
            "Episode 300: Total Reward = -18.0, Avg Reward = -16.40, Epsilon = 0.222\n",
            "Episode 301: Total Reward = -17.0, Avg Reward = -17.20, Epsilon = 0.221\n",
            "Episode 302: Total Reward = -17.0, Avg Reward = -17.80, Epsilon = 0.220\n",
            "Episode 303: Total Reward = -18.0, Avg Reward = -17.80, Epsilon = 0.219\n",
            "Episode 304: Total Reward = -16.0, Avg Reward = -17.20, Epsilon = 0.218\n",
            "Episode 305: Total Reward = -21.0, Avg Reward = -17.80, Epsilon = 0.217\n",
            "Episode 306: Total Reward = -13.0, Avg Reward = -17.00, Epsilon = 0.216\n",
            "Episode 307: Total Reward = -9.0, Avg Reward = -15.40, Epsilon = 0.215\n",
            "Episode 308: Total Reward = -11.0, Avg Reward = -14.00, Epsilon = 0.214\n",
            "Episode 309: Total Reward = -9.0, Avg Reward = -12.60, Epsilon = 0.212\n",
            "Episode 310: Total Reward = -12.0, Avg Reward = -10.80, Epsilon = 0.211\n",
            "Episode 311: Total Reward = -18.0, Avg Reward = -11.80, Epsilon = 0.210\n",
            "Episode 312: Total Reward = -14.0, Avg Reward = -12.80, Epsilon = 0.209\n",
            "Episode 313: Total Reward = -14.0, Avg Reward = -13.40, Epsilon = 0.208\n",
            "Episode 314: Total Reward = -7.0, Avg Reward = -13.00, Epsilon = 0.207\n",
            "Episode 315: Total Reward = -16.0, Avg Reward = -13.80, Epsilon = 0.206\n",
            "Episode 316: Total Reward = -19.0, Avg Reward = -14.00, Epsilon = 0.205\n",
            "Episode 317: Total Reward = -15.0, Avg Reward = -14.20, Epsilon = 0.204\n",
            "Episode 318: Total Reward = -9.0, Avg Reward = -13.20, Epsilon = 0.203\n",
            "Episode 319: Total Reward = -10.0, Avg Reward = -13.80, Epsilon = 0.202\n",
            "Episode 320: Total Reward = -12.0, Avg Reward = -13.00, Epsilon = 0.201\n",
            "Episode 321: Total Reward = -15.0, Avg Reward = -12.20, Epsilon = 0.200\n",
            "Episode 322: Total Reward = -16.0, Avg Reward = -12.40, Epsilon = 0.199\n",
            "Episode 323: Total Reward = -20.0, Avg Reward = -14.60, Epsilon = 0.198\n",
            "Episode 324: Total Reward = -12.0, Avg Reward = -15.00, Epsilon = 0.197\n",
            "Episode 325: Total Reward = -13.0, Avg Reward = -15.20, Epsilon = 0.196\n",
            "Episode 326: Total Reward = -13.0, Avg Reward = -14.80, Epsilon = 0.195\n",
            "Episode 327: Total Reward = -9.0, Avg Reward = -13.40, Epsilon = 0.194\n",
            "Episode 328: Total Reward = -15.0, Avg Reward = -12.40, Epsilon = 0.193\n",
            "Episode 329: Total Reward = -11.0, Avg Reward = -12.20, Epsilon = 0.192\n",
            "Episode 330: Total Reward = -11.0, Avg Reward = -11.80, Epsilon = 0.191\n",
            "Episode 331: Total Reward = -16.0, Avg Reward = -12.40, Epsilon = 0.190\n",
            "Episode 332: Total Reward = -17.0, Avg Reward = -14.00, Epsilon = 0.189\n",
            "Episode 333: Total Reward = -11.0, Avg Reward = -13.20, Epsilon = 0.188\n",
            "Episode 334: Total Reward = -13.0, Avg Reward = -13.60, Epsilon = 0.187\n",
            "Episode 335: Total Reward = -9.0, Avg Reward = -13.20, Epsilon = 0.187\n",
            "Episode 336: Total Reward = -4.0, Avg Reward = -10.80, Epsilon = 0.186\n",
            "Episode 337: Total Reward = -15.0, Avg Reward = -10.40, Epsilon = 0.185\n",
            "Episode 338: Total Reward = -16.0, Avg Reward = -11.40, Epsilon = 0.184\n",
            "Episode 339: Total Reward = -11.0, Avg Reward = -11.00, Epsilon = 0.183\n",
            "Episode 340: Total Reward = -16.0, Avg Reward = -12.40, Epsilon = 0.182\n",
            "Episode 341: Total Reward = -12.0, Avg Reward = -14.00, Epsilon = 0.181\n",
            "Episode 342: Total Reward = -14.0, Avg Reward = -13.80, Epsilon = 0.180\n",
            "Episode 343: Total Reward = -15.0, Avg Reward = -13.60, Epsilon = 0.179\n",
            "Episode 344: Total Reward = -12.0, Avg Reward = -13.80, Epsilon = 0.178\n",
            "Episode 345: Total Reward = -18.0, Avg Reward = -14.20, Epsilon = 0.177\n",
            "Episode 346: Total Reward = -15.0, Avg Reward = -14.80, Epsilon = 0.177\n",
            "Episode 347: Total Reward = -9.0, Avg Reward = -13.80, Epsilon = 0.176\n",
            "Episode 348: Total Reward = -10.0, Avg Reward = -12.80, Epsilon = 0.175\n",
            "Episode 349: Total Reward = -19.0, Avg Reward = -14.20, Epsilon = 0.174\n",
            "Episode 350: Total Reward = -14.0, Avg Reward = -13.40, Epsilon = 0.173\n",
            "Episode 351: Total Reward = -17.0, Avg Reward = -13.80, Epsilon = 0.172\n",
            "Episode 352: Total Reward = -11.0, Avg Reward = -14.20, Epsilon = 0.171\n",
            "Episode 353: Total Reward = -18.0, Avg Reward = -15.80, Epsilon = 0.170\n",
            "Episode 354: Total Reward = -16.0, Avg Reward = -15.20, Epsilon = 0.170\n",
            "Episode 355: Total Reward = -15.0, Avg Reward = -15.40, Epsilon = 0.169\n",
            "Episode 356: Total Reward = -17.0, Avg Reward = -15.40, Epsilon = 0.168\n",
            "Episode 357: Total Reward = -11.0, Avg Reward = -15.40, Epsilon = 0.167\n",
            "Episode 358: Total Reward = -16.0, Avg Reward = -15.00, Epsilon = 0.166\n",
            "Episode 359: Total Reward = -14.0, Avg Reward = -14.60, Epsilon = 0.165\n",
            "Episode 360: Total Reward = -6.0, Avg Reward = -12.80, Epsilon = 0.165\n",
            "Episode 361: Total Reward = -11.0, Avg Reward = -11.60, Epsilon = 0.164\n",
            "Episode 362: Total Reward = -14.0, Avg Reward = -12.20, Epsilon = 0.163\n",
            "Episode 363: Total Reward = -13.0, Avg Reward = -11.60, Epsilon = 0.162\n",
            "Episode 364: Total Reward = -12.0, Avg Reward = -11.20, Epsilon = 0.161\n",
            "Episode 365: Total Reward = -14.0, Avg Reward = -12.80, Epsilon = 0.160\n",
            "Episode 366: Total Reward = -14.0, Avg Reward = -13.40, Epsilon = 0.160\n",
            "Episode 367: Total Reward = -6.0, Avg Reward = -11.80, Epsilon = 0.159\n",
            "Episode 368: Total Reward = -17.0, Avg Reward = -12.60, Epsilon = 0.158\n",
            "Episode 369: Total Reward = -10.0, Avg Reward = -12.20, Epsilon = 0.157\n",
            "Episode 370: Total Reward = -12.0, Avg Reward = -11.80, Epsilon = 0.157\n",
            "Episode 371: Total Reward = -10.0, Avg Reward = -11.00, Epsilon = 0.156\n",
            "Episode 372: Total Reward = -17.0, Avg Reward = -13.20, Epsilon = 0.155\n",
            "Episode 373: Total Reward = -18.0, Avg Reward = -13.40, Epsilon = 0.154\n",
            "Episode 374: Total Reward = -13.0, Avg Reward = -14.00, Epsilon = 0.153\n",
            "Episode 375: Total Reward = -15.0, Avg Reward = -14.60, Epsilon = 0.153\n",
            "Episode 376: Total Reward = -14.0, Avg Reward = -15.40, Epsilon = 0.152\n",
            "Episode 377: Total Reward = -15.0, Avg Reward = -15.00, Epsilon = 0.151\n",
            "Episode 378: Total Reward = -17.0, Avg Reward = -14.80, Epsilon = 0.150\n",
            "Episode 379: Total Reward = -17.0, Avg Reward = -15.60, Epsilon = 0.150\n",
            "Episode 380: Total Reward = -5.0, Avg Reward = -13.60, Epsilon = 0.149\n",
            "Episode 381: Total Reward = -14.0, Avg Reward = -13.60, Epsilon = 0.148\n",
            "Episode 382: Total Reward = -17.0, Avg Reward = -14.00, Epsilon = 0.147\n",
            "Episode 383: Total Reward = -17.0, Avg Reward = -14.00, Epsilon = 0.147\n",
            "Episode 384: Total Reward = -15.0, Avg Reward = -13.60, Epsilon = 0.146\n",
            "Episode 385: Total Reward = -13.0, Avg Reward = -15.20, Epsilon = 0.145\n",
            "Episode 386: Total Reward = -13.0, Avg Reward = -15.00, Epsilon = 0.144\n",
            "Episode 387: Total Reward = -16.0, Avg Reward = -14.80, Epsilon = 0.144\n",
            "Episode 388: Total Reward = -12.0, Avg Reward = -13.80, Epsilon = 0.143\n",
            "Episode 389: Total Reward = -17.0, Avg Reward = -14.20, Epsilon = 0.142\n",
            "Episode 390: Total Reward = -13.0, Avg Reward = -14.20, Epsilon = 0.142\n",
            "Episode 391: Total Reward = -18.0, Avg Reward = -15.20, Epsilon = 0.141\n",
            "Episode 392: Total Reward = -17.0, Avg Reward = -15.40, Epsilon = 0.140\n",
            "Episode 393: Total Reward = -17.0, Avg Reward = -16.40, Epsilon = 0.139\n",
            "Episode 394: Total Reward = -18.0, Avg Reward = -16.60, Epsilon = 0.139\n",
            "Episode 395: Total Reward = -13.0, Avg Reward = -16.60, Epsilon = 0.138\n",
            "Episode 396: Total Reward = -5.0, Avg Reward = -14.00, Epsilon = 0.137\n",
            "Episode 397: Total Reward = -11.0, Avg Reward = -12.80, Epsilon = 0.137\n",
            "Episode 398: Total Reward = -14.0, Avg Reward = -12.20, Epsilon = 0.136\n",
            "Episode 399: Total Reward = -19.0, Avg Reward = -12.40, Epsilon = 0.135\n",
            "Episode 400: Total Reward = -14.0, Avg Reward = -12.60, Epsilon = 0.135\n",
            "Episode 401: Total Reward = -5.0, Avg Reward = -12.60, Epsilon = 0.134\n",
            "Episode 402: Total Reward = -18.0, Avg Reward = -14.00, Epsilon = 0.133\n",
            "Episode 403: Total Reward = -11.0, Avg Reward = -13.40, Epsilon = 0.133\n",
            "Episode 404: Total Reward = -20.0, Avg Reward = -13.60, Epsilon = 0.132\n",
            "Episode 405: Total Reward = -14.0, Avg Reward = -13.60, Epsilon = 0.131\n",
            "Episode 406: Total Reward = -11.0, Avg Reward = -14.80, Epsilon = 0.131\n",
            "Episode 407: Total Reward = -15.0, Avg Reward = -14.20, Epsilon = 0.130\n",
            "Episode 408: Total Reward = -11.0, Avg Reward = -14.20, Epsilon = 0.129\n",
            "Episode 409: Total Reward = -15.0, Avg Reward = -13.20, Epsilon = 0.129\n",
            "Episode 410: Total Reward = -19.0, Avg Reward = -14.20, Epsilon = 0.128\n",
            "Episode 411: Total Reward = -15.0, Avg Reward = -15.00, Epsilon = 0.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZ37Of_sbSDE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "get_started.ipynb",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}